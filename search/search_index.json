{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u00b6 {\\bf\\large Nice To Meet You} {\\bf\\large Nice To Meet You}","title":"Home"},{"location":"docker/HarborAPI/","text":"1. \u9879\u76ee\u7ba1\u7406 \u00b6 1.1 \u67e5\u770b\u4ed3\u5e93\u4e2d\u9879\u76ee\u8be6\u7ec6\u4fe1\u606f \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects?project_name=guest\" 1.2 \u641c\u7d22\u955c\u50cf \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/search?q=nginx\" 1.3 \u5220\u9664\u9879\u76ee \u00b6 Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}\" 1.4 \u521b\u5efa\u9879\u76ee \u00b6 Bash curl -u \"admin:Harbor12345\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects\" -d @createproject.json createproject.json { \"project_name\": \"testrpo\", \"public\": 0 } 1.5 \u67e5\u770b\u9879\u76ee\u65e5\u5fd7 \u00b6 Bash curl -u \"admin:Harbor12345\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/logs/filter\" -d @log.json log.json { \"username\": \"admin\" } 1.6 \u521b\u5efa\u8d26\u53f7 \u00b6 Bash curl -u \"admin:Harbor12345\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users\" -d @user.json user.json { \"user_id\": 5, \"username\": \"weeknd\", \"email\": \" weeknd.su@hotmail.com \", \"password\": \"12345\", \"realname\": \"weeknd\", \"role_id\": 2 } 1.7 \u83b7\u53d6\u7528\u6237\u4fe1\u606f \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users\" 1.8 \u83b7\u53d6\u5f53\u524d\u7528\u6237\u4fe1\u606f \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/current\" 1.9 \u5220\u9664\u7528\u6237 \u00b6 Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/{user_id}\" 2.0 \u4fee\u6539\u7528\u6237\u5bc6\u7801 \u00b6 Bash curl -u \"admin:Harbor12345\" -X PUT -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/{user_id}/password\" -d @uppwd.json uppwd.json { \"old_password\": \"Harbor123456\", \"new_password\": \"Harbor12345\" } 2.1 \u67e5\u770b\u9879\u76ee\u76f8\u5173\u89d2\u8272 \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/members/\" 2.2 \u9879\u76ee\u6dfb\u52a0\u89d2\u8272 \u00b6 Bash curl -u \"jaymarco:Harbor123456\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/members/\" -d @role.json role.json { \"roles\": [3], \"username\": \"guest\" } 2.3 \u7528weeknd\u7528\u6237\u521b\u5efa\u4e00\u4e2adcos\u9879\u76ee\uff0c\u5e76\u5bf9dcos\u52a0\u4e00\u4e2a\u6743\u9650 \u00b6 Bash curl -u \"weeknd:Harbor123456\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects\" -d @createproject.json 2.4 \u5220\u9664\u9879\u76ee\u4e2d\u7528\u6237\u6743\u9650 \u00b6 Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/members/{user_id}\" 2.5 \u83b7\u53d6\u4e0e\u7528\u6237\u76f8\u5173\u7684\u9879\u76ee\u7f16\u53f7\u548c\u5b58\u50a8\u5e93\u7f16\u53f7 \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/statistics\" has_admin_role \uff1a0 \u666e\u901a\u7528\u6237 has_admin_role \uff1a1 \u7ba1\u7406\u5458 Bash curl -u \"admin:Harbor12345\" -X PUT -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/{user_id}/sysadmin \" -d @chgrole.json chgrole.json { \"has_admin_role\": 1 } 2.7 \u67e5\u8be2\u955c\u50cf \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/repositories?project_id={project_id}&q=dcos%2Fcentos\" 2.8 \u5220\u9664\u955c\u50cf \u00b6 Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/repositories?repo_name=dcos%2Fetcd \" 2.9 \u83b7\u53d6\u955c\u50cf\u6807\u7b7e \u00b6 Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/repositories/tags?repo_name=dcos%2Fcentos\"","title":"2. HarborAPI"},{"location":"docker/HarborAPI/#1","text":"","title":"1. \u9879\u76ee\u7ba1\u7406"},{"location":"docker/HarborAPI/#11","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects?project_name=guest\"","title":"1.1     \u67e5\u770b\u4ed3\u5e93\u4e2d\u9879\u76ee\u8be6\u7ec6\u4fe1\u606f"},{"location":"docker/HarborAPI/#12","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/search?q=nginx\"","title":"1.2     \u641c\u7d22\u955c\u50cf"},{"location":"docker/HarborAPI/#13","text":"Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}\"","title":"1.3     \u5220\u9664\u9879\u76ee"},{"location":"docker/HarborAPI/#14","text":"Bash curl -u \"admin:Harbor12345\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects\" -d @createproject.json createproject.json { \"project_name\": \"testrpo\", \"public\": 0 }","title":"1.4     \u521b\u5efa\u9879\u76ee"},{"location":"docker/HarborAPI/#15","text":"Bash curl -u \"admin:Harbor12345\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/logs/filter\" -d @log.json log.json { \"username\": \"admin\" }","title":"1.5     \u67e5\u770b\u9879\u76ee\u65e5\u5fd7"},{"location":"docker/HarborAPI/#16","text":"Bash curl -u \"admin:Harbor12345\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users\" -d @user.json user.json { \"user_id\": 5, \"username\": \"weeknd\", \"email\": \" weeknd.su@hotmail.com \", \"password\": \"12345\", \"realname\": \"weeknd\", \"role_id\": 2 }","title":"1.6     \u521b\u5efa\u8d26\u53f7"},{"location":"docker/HarborAPI/#17","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users\"","title":"1.7     \u83b7\u53d6\u7528\u6237\u4fe1\u606f"},{"location":"docker/HarborAPI/#18","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/current\"","title":"1.8     \u83b7\u53d6\u5f53\u524d\u7528\u6237\u4fe1\u606f"},{"location":"docker/HarborAPI/#19","text":"Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/{user_id}\"","title":"1.9     \u5220\u9664\u7528\u6237"},{"location":"docker/HarborAPI/#20","text":"Bash curl -u \"admin:Harbor12345\" -X PUT -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/{user_id}/password\" -d @uppwd.json uppwd.json { \"old_password\": \"Harbor123456\", \"new_password\": \"Harbor12345\" }","title":"2.0     \u4fee\u6539\u7528\u6237\u5bc6\u7801"},{"location":"docker/HarborAPI/#21","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/members/\"","title":"2.1     \u67e5\u770b\u9879\u76ee\u76f8\u5173\u89d2\u8272"},{"location":"docker/HarborAPI/#22","text":"Bash curl -u \"jaymarco:Harbor123456\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/members/\" -d @role.json role.json { \"roles\": [3], \"username\": \"guest\" }","title":"2.2     \u9879\u76ee\u6dfb\u52a0\u89d2\u8272"},{"location":"docker/HarborAPI/#23-weeknddcosdcos","text":"Bash curl -u \"weeknd:Harbor123456\" -X POST -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects\" -d @createproject.json","title":"2.3 \u7528weeknd\u7528\u6237\u521b\u5efa\u4e00\u4e2adcos\u9879\u76ee\uff0c\u5e76\u5bf9dcos\u52a0\u4e00\u4e2a\u6743\u9650"},{"location":"docker/HarborAPI/#24","text":"Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/projects/{project_id}/members/{user_id}\"","title":"2.4     \u5220\u9664\u9879\u76ee\u4e2d\u7528\u6237\u6743\u9650"},{"location":"docker/HarborAPI/#25","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/statistics\" has_admin_role \uff1a0 \u666e\u901a\u7528\u6237 has_admin_role \uff1a1 \u7ba1\u7406\u5458 Bash curl -u \"admin:Harbor12345\" -X PUT -H \"Content-Type: application/json\" \"https://192.168.1.100/api/users/{user_id}/sysadmin \" -d @chgrole.json chgrole.json { \"has_admin_role\": 1 }","title":"2.5     \u83b7\u53d6\u4e0e\u7528\u6237\u76f8\u5173\u7684\u9879\u76ee\u7f16\u53f7\u548c\u5b58\u50a8\u5e93\u7f16\u53f7"},{"location":"docker/HarborAPI/#27","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/repositories?project_id={project_id}&q=dcos%2Fcentos\"","title":"2.7     \u67e5\u8be2\u955c\u50cf"},{"location":"docker/HarborAPI/#28","text":"Bash curl -u \"admin:Harbor12345\" -X DELETE -H \"Content-Type: application/json\" \"https://192.168.1.100/api/repositories?repo_name=dcos%2Fetcd \"","title":"2.8    \u5220\u9664\u955c\u50cf"},{"location":"docker/HarborAPI/#29","text":"Bash curl -u \"admin:Harbor12345\" -X GET -H \"Content-Type: application/json\" \"https://192.168.1.100/api/repositories/tags?repo_name=dcos%2Fcentos\"","title":"2.9    \u83b7\u53d6\u955c\u50cf\u6807\u7b7e"},{"location":"docker/kafka/","text":"\u90e8\u7f72\u5355\u5b9e\u4f8bkafka \u00b6 1. \u62c9\u53bbkafka\u548czookeeper\u955c\u50cf \u00b6 docker pull wurstmeister/zookeeper docker pull wurstmeister/kafka 2. \u542f\u52a8zookeeper \u00b6 docker run -d --name kafka-zookeeper -p 2181:2181 --volume /etc/localtime:/etc/localtime wurstmeister/zookeeper:latest 3. \u542f\u52a8kafka \u00b6 docker run -d --name kafka -p 9092:9092 --link kafka-zookeeper --env KAFKA_ZOOKEEPER_CONNECT=kafka-zookeeper:2181 --env KAFKA_ADVERTISED_HOST_NAME=localhost --env KAFKA_ADVERTISED_PORT=9092 --volume /etc/localtime:/etc/localtime wurstmeister/kafka:latest 4. \u542f\u52a8\u7b2c\u4e00\u4e2a\u7ec8\u7aef \u00b6 docker exec -it kafka /bin/bash docker exec -it kafka /bin/bash 5. \u67e5\u770btopic \u00b6 cd opt/kafka_2.11-2.0.0/ bin/kafka-topics.sh --list --zookeeper kafka-zookeeper:2181 6. \u521b\u5efa\u4e00\u4e2atopic \u00b6 bin/kafka-topics.sh --create --zookeeper kafka-zookeeper:2181 --replication-factor 1 --partitions 1 --topic mykafka Created topic \"mykafka\". 7. \u521b\u5efa\u4e00\u4e2a\u751f\u4ea7\u8005 \u00b6 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mykafka > NOTE :waiting input message,once you input message in the producer terminal,you will see the same message on your consumer terminal 8. \u6253\u5f00\u53e6\u4e00\u4e2akafka\u7ec8\u7aef \u00b6 docker exec -it kafka /bin/bash 9. \u518d\u6b21\u68c0\u67e5topic \u00b6 pwd /opt/kafka_2.11-2.0.0 bash-4.4# bin/kafka-topics.sh --list --zookeeper kafka-zookeeper:2181 mykafka 10. \u521b\u5efa\u4e00\u4e2aconsumer \u00b6 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mykafka --from-beginning 11. \u6d4b\u8bd5producer\u5230consumer \u00b6 11.1 \u751f\u4ea7\u8005\u7ec8\u7aef\u8f93\u5165\u6d88\u606f \u00b6 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mykafka \\>i'm weeknd 11.2 \u68c0\u67e5\u6d88\u8d39\u8005\u7ec8\u7aef \u00b6 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mykafka --from-beginning i'm weeknd Finished\uff01","title":"1. kafak"},{"location":"docker/kafka/#kafka","text":"","title":"\u90e8\u7f72\u5355\u5b9e\u4f8bkafka"},{"location":"docker/kafka/#1-kafkazookeeper","text":"docker pull wurstmeister/zookeeper docker pull wurstmeister/kafka","title":"1. \u62c9\u53bbkafka\u548czookeeper\u955c\u50cf"},{"location":"docker/kafka/#2-zookeeper","text":"docker run -d --name kafka-zookeeper -p 2181:2181 --volume /etc/localtime:/etc/localtime wurstmeister/zookeeper:latest","title":"2. \u542f\u52a8zookeeper"},{"location":"docker/kafka/#3-kafka","text":"docker run -d --name kafka -p 9092:9092 --link kafka-zookeeper --env KAFKA_ZOOKEEPER_CONNECT=kafka-zookeeper:2181 --env KAFKA_ADVERTISED_HOST_NAME=localhost --env KAFKA_ADVERTISED_PORT=9092 --volume /etc/localtime:/etc/localtime wurstmeister/kafka:latest","title":"3. \u542f\u52a8kafka"},{"location":"docker/kafka/#4","text":"docker exec -it kafka /bin/bash docker exec -it kafka /bin/bash","title":"4. \u542f\u52a8\u7b2c\u4e00\u4e2a\u7ec8\u7aef"},{"location":"docker/kafka/#5-topic","text":"cd opt/kafka_2.11-2.0.0/ bin/kafka-topics.sh --list --zookeeper kafka-zookeeper:2181","title":"5. \u67e5\u770btopic"},{"location":"docker/kafka/#6-topic","text":"bin/kafka-topics.sh --create --zookeeper kafka-zookeeper:2181 --replication-factor 1 --partitions 1 --topic mykafka Created topic \"mykafka\".","title":"6. \u521b\u5efa\u4e00\u4e2atopic"},{"location":"docker/kafka/#7","text":"bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mykafka > NOTE :waiting input message,once you input message in the producer terminal,you will see the same message on your consumer terminal","title":"7. \u521b\u5efa\u4e00\u4e2a\u751f\u4ea7\u8005"},{"location":"docker/kafka/#8-kafka","text":"docker exec -it kafka /bin/bash","title":"8. \u6253\u5f00\u53e6\u4e00\u4e2akafka\u7ec8\u7aef"},{"location":"docker/kafka/#9-topic","text":"pwd /opt/kafka_2.11-2.0.0 bash-4.4# bin/kafka-topics.sh --list --zookeeper kafka-zookeeper:2181 mykafka","title":"9. \u518d\u6b21\u68c0\u67e5topic"},{"location":"docker/kafka/#10-consumer","text":"bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mykafka --from-beginning","title":"10. \u521b\u5efa\u4e00\u4e2aconsumer"},{"location":"docker/kafka/#11-producerconsumer","text":"","title":"11. \u6d4b\u8bd5producer\u5230consumer"},{"location":"docker/kafka/#111","text":"bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mykafka \\>i'm weeknd","title":"11.1 \u751f\u4ea7\u8005\u7ec8\u7aef\u8f93\u5165\u6d88\u606f"},{"location":"docker/kafka/#112","text":"bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mykafka --from-beginning i'm weeknd Finished\uff01","title":"11.2 \u68c0\u67e5\u6d88\u8d39\u8005\u7ec8\u7aef"},{"location":"install/github-pages/","text":"","title":"2. ETCD"},{"location":"install/http-server/","text":"","title":"3. http & https"},{"location":"install/local/","text":"","title":"1. k8s Install"},{"location":"kubernetes/Installing/","text":"Installing k8s HA \u00b6 1. \u5f00\u542fIPVS\uff08\u5f00\u673a\u9700\u8981\u518d\u6b21\u52a0\u8f7d\uff09 \u00b6 Bash modprobe ip_vs modprobe ip_vs modprobe ip_vs_rr modprobe ip_vs_wrr modprobe ip_vs_sh modprobe nf_conntrack_ipv4 2. \u5b89\u88c5etables\u548ckeepalived \u00b6 Bash yum install -y ebtables keepalived 3. \u4fee\u6539/etc/hosts \u00b6 Bash cat >>/etc/hosts <<EOF 172.16.5.70 master01 172.16.5.71 master02 172.16.5.72 master03 172.16.5.90 vip-k8s EOF 4. \u4fee\u6539\u4e3b\u673a\u540d \u00b6 Master01 hostnamectl set-hostname master01 Master02 hostnamectl set-hostname master03 Master03 hostnamectl set-hostname master03 5. \u5173\u95edselinux \u00b6 Bash setenforce 0 sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config 6. \u5173\u95edswap \u00b6 Bash swapoff -a vi /etc/fstab #/dev/vdb swap swap defaults 0 0 swapoff -a :\u4e3a \u4e34\u65f6 \u5173\u95ed swap vi /etc/fstab :\u6ce8\u91ca\u6389 swap \u542f\u52a8\u6302\u8f7d\u9879\u4e3a \u6c38\u4e45 \u5173\u95ed 7. \u4fee\u6539iptables \u00b6 Docker\u4ece1.13\u7248\u672c\u5f00\u59cb\u8c03\u6574\u4e86\u9ed8\u8ba4\u7684\u9632\u706b\u5899\u89c4\u5219\uff0c\u7981\u7528\u4e86iptables filter\u8868\u4e2dFOWARD\u94fe\uff0c\u8fd9\u6837\u4f1a\u5f15\u8d77Kubernetes\u96c6\u7fa4\u4e2d\u8de8Node\u7684Pod\u65e0\u6cd5\u901a\u4fe1\u3002 Bash iptables -P FORWARD ACCEPT 8. \u5173\u95edfirewalld \u00b6 Bash systemctl stop firewalld systemctl disable firewalld 9. \u914d\u7f6e\u8f6c\u53d1 \u00b6 Bash cat <<EOF > /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF 10. \u914d\u7f6e\u4e92\u4fe1 \u00b6 10.1 \u751f\u6210key \u00b6 \u5728\u6240\u6709\u7684master\u4e0a\u6267\u884c\uff0c\u4e00\u8def\u56de\u8f66\u5373\u53ef\uff01 Bash ssh-keygen -t rsa 10.2 \u6dfb\u52a0\u516c\u94a5 \u00b6 \u5728\u6240\u6709\u7684master\u4e0a\u6267\u884c\uff01 Bash ssh-copy-id -i .ssh/id_rsa.pub root@master01 ssh-copy-id -i .ssh/id_rsa.pub root@master02 ssh-copy-id -i .ssh/id_rsa.pub root@master03 11. \u5b89\u88c5Docker \u00b6 11.1 \u5378\u8f7d\u5df2\u5b58\u5728\u7684docker \u00b6 Bash yum remove docker docker-common docker-selinux docker-engine 11.2 \u5b89\u88c5docker-ce-selinux \u00b6 Bash yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm -y 11.3 \u5b89\u88c5docker-ce \u00b6 Bash yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm -y 12. \u914d\u7f6e&\u542f\u52a8docker \u00b6 12.1 \u4fee\u6539\u542f\u52a8\u53c2\u6570 \u00b6 Bash vi /usr/lib/systemd/system/docker.service ExecStart = /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --registry-mirror = https://ms3cfraz.mirror.aliyuncs.com 12.2 \u542f\u52a8docker \u00b6 Bash systemctl enable docker && systemctl restart docker docker\u542f\u52a8\u5931\u8d25 \u9519\u8bef\u4fe1\u606f \uff1a9\u6708 30 17:40:29 cicd dockerd[22642]: Error starting daemon: error initializing graphdriver: driver not supported \u89e3\u51b3\u65b9\u6cd5 \uff1a\u4fee\u6539daemon.json /etc/docker/daemon.json { \"storage-driver\" : \"overlay2\" , \"storage-opts\" : [ \"overlay2.override_kernel_check=true\" ] } 13 \u5b89\u88c5kubernetes \u00b6 13.1 \u914d\u7f6ekubernete Yum\u6e90 \u00b6 Bash cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 13.2 \u5b89\u88c5k8s \u00b6 Bash yum install -y kubelet kubeadm kubectl ipvsadm 13.2 \u8bbe\u7f6ekubelet\u5f00\u673a\u542f\u52a8 \u00b6 Bash systemctl enable kubelet.service 13.3 \u83b7\u53d6\u9700\u8981\u7684\u955c\u50cf\u7248\u672c \u00b6 Bash kubeadm config images list k8s.gcr.io/kube-apiserver:v1.12.2 k8s.gcr.io/kube-controller-manager:v1.12.2 k8s.gcr.io/kube-scheduler:v1.12.2 k8s.gcr.io/kube-proxy:v1.12.2 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:3.2.24 k8s.gcr.io/coredns:1.2.2 13.4 \u4eceharbor\u62c9\u53d6\u955c\u50cf \u00b6 image.sh #!/bin/bash images =( kube-apiserver:v1.12.2 kkube-controller-manager:v1.12.2 kube-scheduler:v1.12.2 kube-proxy:v1.12.2 pause:3.1 etcd:3.2.24 coredns:1.2.2 ) for imageName in ${ images [@] } ; do docker pull reg.xxxx.xxx/k8s.gcr.io/ $imageName docker tag reg.xxxx.xxx/k8s.gcr.io/ $imageName k8s.gcr.io/ $imageName docker rmi reg.xxxx.xxx/k8s.gcr.io/ $imageName done 13.5 \u62c9\u53d6\u955c\u50cf \u00b6 Bash sh image.sh 14 \u7f16\u5199\u96c6\u7fa4\u4fe1\u606f \u00b6 CLUSTER_INFO # MASTER01 IP CP0_IP = 172 .16.5.70 # MASTER01 NAME CP0_HOSTNAME = master01 # MASTER02 IP CP1_IP = 172 .16.5.71 # MASTER02 IP CP1_HOSTNAME = master02 # MASTER03 IP CP2_IP = 172 .16.5.72 # MASTER03 IP CP2_HOSTNAME = master03 # VIP(KEEPALIVED) VIP = 172 .16.5.90 # \u83b7\u53d6NIC\uff08\u8d70\u6d41\u91cf\u7684\u7f51\u5361\u540d\uff09 NET_IF = eth0 # POD IP CIDR = 172 .20.0.0/16 # SERVICE IP SVCRANGE = 172 .30.0.0/16 \u8fd9\u91cc\u7f16\u5199\u4e86\u4e00\u4e2a\u6279\u91cf\u83b7\u53d6NIC\u548cIP\u7684\u547d\u4ee4\uff0c\u53ef\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u505a\u4fee\u6539 export NODES=\"master01 master02 master03\" # \u83b7\u53d6\u8d70\u6d41\u91cf\u7f51\u5361ip ip route get 172.16.5.0 | awk '{print $NF; exit}' # \u83b7\u53d6\u6240\u6709master\u8282\u70b9\u7684ip ssh ${NODE} \"ip route get 172.16.5.0\" | awk '{print $NF; exit}' # \u83b7\u53d6NIC\uff08\u8d70\u6d41\u91cf\u7684\u7f51\u5361\u540d\uff09 ssh ${NODE} \"ip route get 172.16.5.0\" | awk '{print $4; exit}' 15 \u914d\u7f6e\u96c6\u7fa4\u5e76\u542f\u52a8 \u00b6 15.1 \u7f16\u5199Calico Yaml \u00b6 calico.yaml # Calico Version v3.2.4 # https://docs.projectcalico.org/v3.2/releases#v3.2.4 # This manifest includes the following component versions: # calico/node:v3.2.4 # calico/cni:v3.2.4 # This ConfigMap is used to configure a self-hosted Calico installation. kind: ConfigMap apiVersion: v1 metadata: name: calico-config namespace: kube-system data: # To enable Typha, set this to \"calico-typha\" *and* set a non-zero value for Typha replicas # below. We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is # essential. typha_service_name: \"none\" # Configure the Calico backend to use. calico_backend: \"bird\" # Configure the MTU to use veth_mtu: \"1440\" # The CNI network configuration to install on each node. The special # values in this config will be automatically populated. cni_network_config: | - { \"name\" : \"k8s-pod-network\" , \"cniVersion\" : \"0.3.0\" , \"plugins\" : [ { \"type\" : \"calico\" , \"log_level\" : \"info\" , \"datastore_type\" : \"kubernetes\" , \"nodename\" : \"__KUBERNETES_NODE_NAME__\" , \"mtu\" : __CNI_MTU__, \"ipam\" : { \"type\" : \"host-local\" , \"subnet\" : \"usePodCidr\" } , \"policy\" : { \"type\" : \"k8s\" } , \"kubernetes\" : { \"kubeconfig\" : \"__KUBECONFIG_FILEPATH__\" } } , { \"type\" : \"portmap\" , \"snat\" : true, \"capabilities\" : { \"portMappings\" : true } } ] } --- # This manifest creates a Service, which will be backed by Calico's Typha daemon. # Typha sits in between Felix and the API server, reducing Calico's load on the API server. apiVersion: v1 kind: Service metadata: name: calico-typha namespace: kube-system labels: k8s-app: calico-typha spec: ports: - port: 5473 protocol: TCP targetPort: calico-typha name: calico-typha selector: k8s-app: calico-typha --- # This manifest creates a Deployment of Typha to back the above service. apiVersion: apps/v1beta1 kind: Deployment metadata: name: calico-typha namespace: kube-system labels: k8s-app: calico-typha spec: # Number of Typha replicas. To enable Typha, set this to a non-zero value *and* set the # typha_service_name variable in the calico-config ConfigMap above. # # We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is essential # (when using the Kubernetes datastore). Use one replica for every 100-200 nodes. In # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade. replicas: 0 revisionHistoryLimit: 2 template: metadata: labels: k8s-app: calico-typha annotations: # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical # add-on, ensuring it gets priority scheduling and that its resources are reserved # if it ever gets evicted. scheduler.alpha.kubernetes.io/critical-pod: '' spec: nodeSelector: beta.kubernetes.io/os: linux hostNetwork: true tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists # Since Calico can't network a pod until Typha is up, we need to run Typha itself # as a host-networked pod. serviceAccountName: calico-node containers: - image: quay.io/calico/typha:v3.2.4 name: calico-typha ports: - containerPort: 5473 name: calico-typha protocol: TCP env: # Enable \"info\" logging by default. Can be set to \"debug\" to increase verbosity. - name: TYPHA_LOGSEVERITYSCREEN value: \"info\" # Disable logging to file and syslog since those don't make sense in Kubernetes. - name: TYPHA_LOGFILEPATH value: \"none\" - name: TYPHA_LOGSEVERITYSYS value: \"none\" # Monitor the Kubernetes API to find the number of running instances and rebalance # connections. - name: TYPHA_CONNECTIONREBALANCINGMODE value: \"kubernetes\" - name: TYPHA_DATASTORETYPE value: \"kubernetes\" - name: TYPHA_HEALTHENABLED value: \"true\" # Uncomment these lines to enable prometheus metrics. Since Typha is host-networked, # this opens a port on the host, which may need to be secured. #- name: TYPHA_PROMETHEUSMETRICSENABLED # value: \"true\" #- name: TYPHA_PROMETHEUSMETRICSPORT # value: \"9093\" livenessProbe: exec: command: - calico-typha - check - liveness periodSeconds: 30 initialDelaySeconds: 30 readinessProbe: exec: command: - calico-typha - check - readiness periodSeconds: 10 --- # This manifest installs the calico/node container, as well # as the Calico CNI plugins and network config on # each master and worker node in a Kubernetes cluster. kind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. scheduler.alpha.kubernetes.io/critical-pod: '' spec: nodeSelector: beta.kubernetes.io/os: linux hostNetwork: true tolerations: # Make sure calico-node gets scheduled on all nodes. - effect: NoSchedule operator: Exists # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists - effect: NoExecute operator: Exists serviceAccountName: calico-node # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods. terminationGracePeriodSeconds: 0 containers: # Runs calico/node container on each Kubernetes node. This # container programs network policy and routes on each # host. - name: calico-node image: quay.io/calico/node:v3.2.4 env: # Use Kubernetes API as the backing datastore. - name: DATASTORE_TYPE value: \"kubernetes\" # Typha support: controlled by the ConfigMap. - name: FELIX_TYPHAK8SSERVICENAME valueFrom: configMapKeyRef: name: calico-config key: typha_service_name # Wait for the datastore. - name: WAIT_FOR_DATASTORE value: \"true\" # Set based on the k8s node name. - name: NODENAME valueFrom: fieldRef: fieldPath: spec.nodeName # Choose the backend to use. - name: CALICO_NETWORKING_BACKEND valueFrom: configMapKeyRef: name: calico-config key: calico_backend # Cluster type to identify the deployment type - name: CLUSTER_TYPE value: \"k8s,bgp\" # Auto-detect the BGP IP address. - name: IP value: \"autodetect\" # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Always\" # Enable IP-in-IP within Felix. - name: FELIX_IPINIPENABLED value: \"true\" # Set MTU for tunnel device used if ipip is enabled - name: FELIX_IPINIPMTU valueFrom: configMapKeyRef: name: calico-config key: veth_mtu # The default IPv4 pool to create on startup if none exists. Pod IPs will be # chosen from this range. Changing this value after installation will have # no effect. This should fall within `--cluster-cidr`. - name: CALICO_IPV4POOL_CIDR value: \"172.20.0.0/16\" # Disable file logging so `kubectl logs` works. - name: CALICO_DISABLE_FILE_LOGGING value: \"true\" # Set Felix endpoint to host default action to ACCEPT. - name: FELIX_DEFAULTENDPOINTTOHOSTACTION value: \"ACCEPT\" # Disable IPv6 on Kubernetes. - name: FELIX_IPV6SUPPORT value: \"false\" # Set Felix logging to \"info\" - name: FELIX_LOGSEVERITYSCREEN value: \"info\" - name: FELIX_HEALTHENABLED value: \"true\" securityContext: privileged: true resources: requests: cpu: 250m livenessProbe: httpGet: path: /liveness port: 9099 host: localhost periodSeconds: 10 initialDelaySeconds: 10 failureThreshold: 6 readinessProbe: exec: command: - /bin/calico-node - -bird-ready - -felix-ready periodSeconds: 10 volumeMounts: - mountPath: /lib/modules name: lib-modules readOnly: true - mountPath: /var/run/calico name: var-run-calico readOnly: false - mountPath: /var/lib/calico name: var-lib-calico readOnly: false # This container installs the Calico CNI binaries # and CNI network config file on each node. - name: install-cni image: quay.io/calico/cni:v3.2.4 command: [ \"/install-cni.sh\" ] env: # Name of the CNI config file to create. - name: CNI_CONF_NAME value: \"10-calico.conflist\" # Set the hostname based on the k8s node name. - name: KUBERNETES_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName # The CNI network config to install on each node. - name: CNI_NETWORK_CONFIG valueFrom: configMapKeyRef: name: calico-config key: cni_network_config # CNI MTU Config variable - name: CNI_MTU valueFrom: configMapKeyRef: name: calico-config key: veth_mtu volumeMounts: - mountPath: /host/opt/cni/bin name: cni-bin-dir - mountPath: /host/etc/cni/net.d name: cni-net-dir volumes: # Used by calico/node. - name: lib-modules hostPath: path: /lib/modules - name: var-run-calico hostPath: path: /var/run/calico - name: var-lib-calico hostPath: path: /var/lib/calico # Used to install CNI. - name: cni-bin-dir hostPath: path: /opt/cni/bin - name: cni-net-dir hostPath: path: /etc/cni/net.d --- apiVersion: v1 kind: ServiceAccount metadata: name: calico-node namespace: kube-system --- # Create all the CustomResourceDefinitions needed for # Calico policy and networking mode. apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: felixconfigurations.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: FelixConfiguration plural: felixconfigurations singular: felixconfiguration --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: bgppeers.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: BGPPeer plural: bgppeers singular: bgppeer --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: bgpconfigurations.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: BGPConfiguration plural: bgpconfigurations singular: bgpconfiguration --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ippools.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: IPPool plural: ippools singular: ippool --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: hostendpoints.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: HostEndpoint plural: hostendpoints singular: hostendpoint --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: clusterinformations.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: ClusterInformation plural: clusterinformations singular: clusterinformation --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: globalnetworkpolicies.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: GlobalNetworkPolicy plural: globalnetworkpolicies singular: globalnetworkpolicy --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: globalnetworksets.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: GlobalNetworkSet plural: globalnetworksets singular: globalnetworkset --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networkpolicies.crd.projectcalico.org spec: scope: Namespaced group: crd.projectcalico.org version: v1 names: kind: NetworkPolicy plural: networkpolicies singular: networkpolicy rbac.yaml # Calico Version v3.2.4 # https://docs.projectcalico.org/v3.2/releases#v3.2.4 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: calico-node rules: - apiGroups: [ \"\" ] resources: - namespaces - serviceaccounts verbs: - get - list - watch - apiGroups: [ \"\" ] resources: - pods/status verbs: - update - apiGroups: [ \"\" ] resources: - pods verbs: - get - list - watch - patch - apiGroups: [ \"\" ] resources: - services verbs: - get - apiGroups: [ \"\" ] resources: - endpoints verbs: - get - apiGroups: [ \"\" ] resources: - nodes verbs: - get - list - update - watch - apiGroups: [ \"extensions\" ] resources: - networkpolicies verbs: - get - list - watch - apiGroups: [ \"networking.k8s.io\" ] resources: - networkpolicies verbs: - watch - list - apiGroups: [ \"crd.projectcalico.org\" ] resources: - globalfelixconfigs - felixconfigurations - bgppeers - globalbgpconfigs - bgpconfigurations - ippools - globalnetworkpolicies - globalnetworksets - networkpolicies - clusterinformations - hostendpoints verbs: - create - get - list - update - watch --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: calico-node roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: calico-node subjects: - kind: ServiceAccount name: calico-node namespace: kube-system 15.2 \u7f16\u5199\u81ea\u52a8\u914d\u7f6e\u811a\u672c \u00b6 \u81ea\u52a8\u914d\u7f6e\u811a\u672c\u8981,\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6,Calico\u653e\u5728\u540c\u7ea7\u76ee\u5f55 setup.sh #/bin/bash function check_parm () { if [ \" ${ 2 } \" == \"\" ] ; then echo -n \" ${ 1 } \" return 1 else return 0 fi } if [ -f ./master-info ] ; then source ./CLUSTER_INFO fi check_parm \"Enter the IP address of master-01: \" ${ CP0_IP } if [ $? -eq 1 ] ; then read CP0_IP fi check_parm \"Enter the Hostname of master-01: \" ${ CP0_HOSTNAME } if [ $? -eq 1 ] ; then read CP0_HOSTNAME fi check_parm \"Enter the IP address of master-02: \" ${ CP1_IP } if [ $? -eq 1 ] ; then read CP1_IP fi check_parm \"Enter the Hostname of master-02: \" ${ CP1_HOSTNAME } if [ $? -eq 1 ] ; then read CP1_HOSTNAME fi check_parm \"Enter the IP address of master-03: \" ${ CP2_IP } if [ $? -eq 1 ] ; then read CP2_IP fi check_parm \"Enter the Hostname of master-03: \" ${ CP2_HOSTNAME } if [ $? -eq 1 ] ; then read CP2_HOSTNAME fi check_parm \"Enter the VIP: \" ${ VIP } if [ $? -eq 1 ] ; then read VIP fi check_parm \"Enter the Net Interface: \" ${ NET_IF } if [ $? -eq 1 ] ; then read NET_IF fi check_parm \"Enter the cluster CIDR: \" ${ CIDR } if [ $? -eq 1 ] ; then read CIDR fi echo \"\"\" cluster-info: master-01: ${ CP0_IP } ${ CP0_HOSTNAME } master-02: ${ CP1_IP } ${ CP1_HOSTNAME } master-03: ${ CP2_IP } ${ CP2_HOSTNAME } VIP: ${ VIP } Net Interface: ${ NET_IF } CIDR: ${ CIDR } \"\"\" echo -n 'Please print \"yes\" to continue or \"no\" to cancle: ' read AGREE while [ \" ${ AGREE } \" ! = \"yes\" ] ; do if [ \" ${ AGREE } \" == \"no\" ] ; then exit 0 ; else echo -n 'Please print \"yes\" to continue or \"no\" to cancle: ' read AGREE fi done mkdir -p ~/ikube/tls HOSTS =( ${ CP0_HOSTNAME } ${ CP1_HOSTNAME } ${ CP2_HOSTNAME } ) IPS =( ${ CP0_IP } ${ CP1_IP } ${ CP2_IP } ) PRIORITY =( 100 50 30 ) STATE =( \"MASTER\" \"BACKUP\" \"BACKUP\" ) HEALTH_CHECK = \"\" for index in 0 1 2 ; do HEALTH_CHECK = ${ HEALTH_CHECK } \"\"\" real_server ${ IPS [ $index ] } 6443 { weight 1 SSL_GET { url { path /healthz status_code 200 } connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } \"\"\" done for index in 0 1 2 ; do host = ${ HOSTS [ ${ index } ] } ip = ${ IPS [ ${ index } ] } echo \"\"\" global_defs { router_id LVS_DEVEL } vrrp_instance VI_1 { state ${ STATE [ ${ index } ] } interface ${ NET_IF } virtual_router_id 80 priority ${ PRIORITY [ ${ index } ] } advert_int 1 authentication { auth_type PASS auth_pass just0kk } virtual_ipaddress { ${ VIP } } } virtual_server ${ VIP } 6443 { delay_loop 6 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP ${ HEALTH_CHECK } } \"\"\" > ~/ikube/keepalived- ${ index } .conf scp ~/ikube/keepalived- ${ index } .conf ${ host } :/etc/keepalived/keepalived.conf ssh ${ host } \" systemctl restart keepalived kubeadm reset -f rm -rf /etc/kubernetes/pki/\" if [ ${ index } -ne 0 ] ; then ETCD_MEMBER = \" ${ ETCD_MEMBER } ,\" ETCD_STATUS = \"existing\" else ETCD_MEMBER = \"\" ETCD_STATUS = \"new\" fi ETCD_MEMBER = \" ${ ETCD_MEMBER }${ host } =https:// ${ ip } :2380\" echo \"\"\" apiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.12.2 apiServerCertSANs: - ${ CP0_IP } - ${ CP1_IP } - ${ CP2_IP } - ${ CP0_HOSTNAME } - ${ CP1_HOSTNAME } - ${ CP2_HOSTNAME } - ${ VIP } kubeProxy: config: mode: ipvs etcd: local: extraArgs: listen-client-urls: https://127.0.0.1:2379,https:// ${ ip } :2379 advertise-client-urls: https:// ${ ip } :2379 listen-peer-urls: https:// ${ ip } :2380 initial-advertise-peer-urls: https:// ${ ip } :2380 initial-cluster: ${ ETCD_MEMBER } initial-cluster-state: ${ ETCD_STATUS } serverCertSANs: - ${ host } - ${ ip } peerCertSANs: - ${ host } - ${ ip } networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: ${ CIDR } serviceSubnet: ${ SVCRANGE } \"\"\" > ~/ikube/kubeadm-config-m ${ index } .yaml scp ~/ikube/kubeadm-config-m ${ index } .yaml ${ host } :/etc/kubernetes/kubeadm-config.yaml done kubeadm init --config /etc/kubernetes/kubeadm-config.yaml mkdir -p $HOME /.kube cp -f /etc/kubernetes/admin.conf ${ HOME } /.kube/config ETCD = ` kubectl get pods -n kube-system 2 > & 1 | grep etcd | awk '{print $3}' ` echo \"Waiting for etcd bootup...\" while [ \" ${ ETCD } \" ! = \"Running\" ] ; do sleep 1 ETCD = ` kubectl get pods -n kube-system 2 > & 1 | grep etcd | awk '{print $3}' ` done for index in 1 2 ; do host = ${ HOSTS [ ${ index } ] } ip = ${ IPS [ ${ index } ] } ssh $host \"mkdir -p /etc/kubernetes/pki/etcd\" ssh $host \"mkdir -p $HOME /.kube\" scp /etc/kubernetes/pki/ca.crt $host :/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $host :/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $host :/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $host :/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $host :/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $host :/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $host :/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $host :/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $host :/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $host :~/.kube/config kubectl exec \\ -n kube-system etcd- ${ CP0_HOSTNAME } -- etcdctl \\ --ca-file /etc/kubernetes/pki/etcd/ca.crt \\ --cert-file /etc/kubernetes/pki/etcd/peer.crt \\ --key-file /etc/kubernetes/pki/etcd/peer.key \\ --endpoints = https:// ${ CP0_IP } :2379 \\ member add ${ host } https:// ${ ip } :2380 ssh ${ host } \" kubeadm alpha phase certs all --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig controller-manager --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig scheduler --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubelet config write-to-disk --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubelet write-env-file --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig kubelet --config /etc/kubernetes/kubeadm-config.yaml systemctl restart kubelet kubeadm alpha phase etcd local --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig all --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase controlplane all --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase mark-master --config /etc/kubernetes/kubeadm-config.yaml\" done kubectl apply -f rbac.yaml kubectl apply -f calico.yaml echo \"Cluster create finished.\" echo \"\"\" [req] distinguished_name = req_distinguished_name prompt = yes [ req_distinguished_name ] countryName = Country Name (2 letter code) countryName_value = CN stateOrProvinceName = State or Province Name (full name) stateOrProvinceName_value = Shanghai localityName = Locality Name (eg, city) localityName_value = Shanghai organizationName = Organization Name (eg, company) organizationName_value = Weeknd organizationalUnitName = Organizational Unit Name (eg, section) organizationalUnitName_value = R & D Department commonName = Common Name (eg, your name or your server\\'s hostname) commonName_value = *.multi.io emailAddress = Email Address emailAddress_value = weeknd.su@hotmail.com \"\"\" > ~/ikube/tls/openssl.cnf openssl req -newkey rsa:4096 -nodes -config ~/ikube/tls/openssl.cnf -days 3650 -x509 -out ~/ikube/tls/tls.crt -keyout ~/ikube/tls/tls.key kubectl create -n kube-system secret tls ssl --cert ~/ikube/tls/tls.crt --key ~/ikube/tls/tls.key for index in 0 1 2 ; do host = ${ HOSTS [ ${ index } ] } ssh ${ host } \"sed -i 's/etcd-servers=https:\\/\\/127.0.0.1:2379/etcd-servers=https:\\/\\/ ${ CP0_IP } :2379,https:\\/\\/ ${ CP1_IP } :2379,https:\\/\\/ ${ CP2_IP } :2379/g' /etc/kubernetes/manifests/kube-apiserver.yaml\" ssh ${ host } \"sed -i 's/ ${ CP0_IP } / ${ VIP } /g' ~/.kube/config\" done echo \"Plugin install finished.\" echo \"Waiting for all pods into 'Running' statu. You can press 'Ctrl + c' to terminate this waiting any time you like.\" POD_UNREADY = ` kubectl get pods -n kube-system 2 > & 1 | awk '{print $3}' | grep -vE 'Running|STATUS' ` NODE_UNREADY = ` kubectl get nodes 2 > & 1 | awk '{print $2}' | grep 'NotReady' ` while [ \" ${ POD_UNREADY } \" ! = \"\" -o \" ${ NODE_UNREADY } \" ! = \"\" ] ; do sleep 1 POD_UNREADY = ` kubectl get pods -n kube-system 2 > & 1 | awk '{print $3}' | grep -vE 'Running|STATUS' ` NODE_UNREADY = ` kubectl get nodes 2 > & 1 | awk '{print $2}' | grep 'NotReady' ` done 15.3 \u8fd0\u884c\u811a\u672c \u00b6 Bash sh setup.sh \u914d\u7f6e\u7ed3\u675f \u00b6","title":"1. Install k8s HA"},{"location":"kubernetes/Installing/#installing-k8s-ha","text":"","title":"Installing k8s HA"},{"location":"kubernetes/Installing/#1-ipvs","text":"Bash modprobe ip_vs modprobe ip_vs modprobe ip_vs_rr modprobe ip_vs_wrr modprobe ip_vs_sh modprobe nf_conntrack_ipv4","title":"1. \u5f00\u542fIPVS\uff08\u5f00\u673a\u9700\u8981\u518d\u6b21\u52a0\u8f7d\uff09"},{"location":"kubernetes/Installing/#2-etableskeepalived","text":"Bash yum install -y ebtables keepalived","title":"2. \u5b89\u88c5etables\u548ckeepalived"},{"location":"kubernetes/Installing/#3-etchosts","text":"Bash cat >>/etc/hosts <<EOF 172.16.5.70 master01 172.16.5.71 master02 172.16.5.72 master03 172.16.5.90 vip-k8s EOF","title":"3. \u4fee\u6539/etc/hosts"},{"location":"kubernetes/Installing/#4","text":"Master01 hostnamectl set-hostname master01 Master02 hostnamectl set-hostname master03 Master03 hostnamectl set-hostname master03","title":"4. \u4fee\u6539\u4e3b\u673a\u540d"},{"location":"kubernetes/Installing/#5-selinux","text":"Bash setenforce 0 sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config","title":"5. \u5173\u95edselinux"},{"location":"kubernetes/Installing/#6-swap","text":"Bash swapoff -a vi /etc/fstab #/dev/vdb swap swap defaults 0 0 swapoff -a :\u4e3a \u4e34\u65f6 \u5173\u95ed swap vi /etc/fstab :\u6ce8\u91ca\u6389 swap \u542f\u52a8\u6302\u8f7d\u9879\u4e3a \u6c38\u4e45 \u5173\u95ed","title":"6. \u5173\u95edswap"},{"location":"kubernetes/Installing/#7-iptables","text":"Docker\u4ece1.13\u7248\u672c\u5f00\u59cb\u8c03\u6574\u4e86\u9ed8\u8ba4\u7684\u9632\u706b\u5899\u89c4\u5219\uff0c\u7981\u7528\u4e86iptables filter\u8868\u4e2dFOWARD\u94fe\uff0c\u8fd9\u6837\u4f1a\u5f15\u8d77Kubernetes\u96c6\u7fa4\u4e2d\u8de8Node\u7684Pod\u65e0\u6cd5\u901a\u4fe1\u3002 Bash iptables -P FORWARD ACCEPT","title":"7. \u4fee\u6539iptables"},{"location":"kubernetes/Installing/#8-firewalld","text":"Bash systemctl stop firewalld systemctl disable firewalld","title":"8. \u5173\u95edfirewalld"},{"location":"kubernetes/Installing/#9","text":"Bash cat <<EOF > /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF","title":"9. \u914d\u7f6e\u8f6c\u53d1"},{"location":"kubernetes/Installing/#10","text":"","title":"10. \u914d\u7f6e\u4e92\u4fe1"},{"location":"kubernetes/Installing/#101-key","text":"\u5728\u6240\u6709\u7684master\u4e0a\u6267\u884c\uff0c\u4e00\u8def\u56de\u8f66\u5373\u53ef\uff01 Bash ssh-keygen -t rsa","title":"10.1 \u751f\u6210key"},{"location":"kubernetes/Installing/#102","text":"\u5728\u6240\u6709\u7684master\u4e0a\u6267\u884c\uff01 Bash ssh-copy-id -i .ssh/id_rsa.pub root@master01 ssh-copy-id -i .ssh/id_rsa.pub root@master02 ssh-copy-id -i .ssh/id_rsa.pub root@master03","title":"10.2 \u6dfb\u52a0\u516c\u94a5"},{"location":"kubernetes/Installing/#11-docker","text":"","title":"11. \u5b89\u88c5Docker"},{"location":"kubernetes/Installing/#111-docker","text":"Bash yum remove docker docker-common docker-selinux docker-engine","title":"11.1 \u5378\u8f7d\u5df2\u5b58\u5728\u7684docker"},{"location":"kubernetes/Installing/#112-docker-ce-selinux","text":"Bash yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm -y","title":"11.2 \u5b89\u88c5docker-ce-selinux"},{"location":"kubernetes/Installing/#113-docker-ce","text":"Bash yum install https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm -y","title":"11.3 \u5b89\u88c5docker-ce"},{"location":"kubernetes/Installing/#12-docker","text":"","title":"12. \u914d\u7f6e&amp;\u542f\u52a8docker"},{"location":"kubernetes/Installing/#121","text":"Bash vi /usr/lib/systemd/system/docker.service ExecStart = /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --registry-mirror = https://ms3cfraz.mirror.aliyuncs.com","title":"12.1 \u4fee\u6539\u542f\u52a8\u53c2\u6570"},{"location":"kubernetes/Installing/#122-docker","text":"Bash systemctl enable docker && systemctl restart docker docker\u542f\u52a8\u5931\u8d25 \u9519\u8bef\u4fe1\u606f \uff1a9\u6708 30 17:40:29 cicd dockerd[22642]: Error starting daemon: error initializing graphdriver: driver not supported \u89e3\u51b3\u65b9\u6cd5 \uff1a\u4fee\u6539daemon.json /etc/docker/daemon.json { \"storage-driver\" : \"overlay2\" , \"storage-opts\" : [ \"overlay2.override_kernel_check=true\" ] }","title":"12.2 \u542f\u52a8docker"},{"location":"kubernetes/Installing/#13-kubernetes","text":"","title":"13 \u5b89\u88c5kubernetes"},{"location":"kubernetes/Installing/#131-kubernete-yum","text":"Bash cat <<EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF","title":"13.1 \u914d\u7f6ekubernete Yum\u6e90"},{"location":"kubernetes/Installing/#132-k8s","text":"Bash yum install -y kubelet kubeadm kubectl ipvsadm","title":"13.2 \u5b89\u88c5k8s"},{"location":"kubernetes/Installing/#132-kubelet","text":"Bash systemctl enable kubelet.service","title":"13.2 \u8bbe\u7f6ekubelet\u5f00\u673a\u542f\u52a8"},{"location":"kubernetes/Installing/#133","text":"Bash kubeadm config images list k8s.gcr.io/kube-apiserver:v1.12.2 k8s.gcr.io/kube-controller-manager:v1.12.2 k8s.gcr.io/kube-scheduler:v1.12.2 k8s.gcr.io/kube-proxy:v1.12.2 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:3.2.24 k8s.gcr.io/coredns:1.2.2","title":"13.3 \u83b7\u53d6\u9700\u8981\u7684\u955c\u50cf\u7248\u672c"},{"location":"kubernetes/Installing/#134-harbor","text":"image.sh #!/bin/bash images =( kube-apiserver:v1.12.2 kkube-controller-manager:v1.12.2 kube-scheduler:v1.12.2 kube-proxy:v1.12.2 pause:3.1 etcd:3.2.24 coredns:1.2.2 ) for imageName in ${ images [@] } ; do docker pull reg.xxxx.xxx/k8s.gcr.io/ $imageName docker tag reg.xxxx.xxx/k8s.gcr.io/ $imageName k8s.gcr.io/ $imageName docker rmi reg.xxxx.xxx/k8s.gcr.io/ $imageName done","title":"13.4 \u4eceharbor\u62c9\u53d6\u955c\u50cf"},{"location":"kubernetes/Installing/#135","text":"Bash sh image.sh","title":"13.5 \u62c9\u53d6\u955c\u50cf"},{"location":"kubernetes/Installing/#14","text":"CLUSTER_INFO # MASTER01 IP CP0_IP = 172 .16.5.70 # MASTER01 NAME CP0_HOSTNAME = master01 # MASTER02 IP CP1_IP = 172 .16.5.71 # MASTER02 IP CP1_HOSTNAME = master02 # MASTER03 IP CP2_IP = 172 .16.5.72 # MASTER03 IP CP2_HOSTNAME = master03 # VIP(KEEPALIVED) VIP = 172 .16.5.90 # \u83b7\u53d6NIC\uff08\u8d70\u6d41\u91cf\u7684\u7f51\u5361\u540d\uff09 NET_IF = eth0 # POD IP CIDR = 172 .20.0.0/16 # SERVICE IP SVCRANGE = 172 .30.0.0/16 \u8fd9\u91cc\u7f16\u5199\u4e86\u4e00\u4e2a\u6279\u91cf\u83b7\u53d6NIC\u548cIP\u7684\u547d\u4ee4\uff0c\u53ef\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u505a\u4fee\u6539 export NODES=\"master01 master02 master03\" # \u83b7\u53d6\u8d70\u6d41\u91cf\u7f51\u5361ip ip route get 172.16.5.0 | awk '{print $NF; exit}' # \u83b7\u53d6\u6240\u6709master\u8282\u70b9\u7684ip ssh ${NODE} \"ip route get 172.16.5.0\" | awk '{print $NF; exit}' # \u83b7\u53d6NIC\uff08\u8d70\u6d41\u91cf\u7684\u7f51\u5361\u540d\uff09 ssh ${NODE} \"ip route get 172.16.5.0\" | awk '{print $4; exit}'","title":"14 \u7f16\u5199\u96c6\u7fa4\u4fe1\u606f"},{"location":"kubernetes/Installing/#15","text":"","title":"15 \u914d\u7f6e\u96c6\u7fa4\u5e76\u542f\u52a8"},{"location":"kubernetes/Installing/#151-calico-yaml","text":"calico.yaml # Calico Version v3.2.4 # https://docs.projectcalico.org/v3.2/releases#v3.2.4 # This manifest includes the following component versions: # calico/node:v3.2.4 # calico/cni:v3.2.4 # This ConfigMap is used to configure a self-hosted Calico installation. kind: ConfigMap apiVersion: v1 metadata: name: calico-config namespace: kube-system data: # To enable Typha, set this to \"calico-typha\" *and* set a non-zero value for Typha replicas # below. We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is # essential. typha_service_name: \"none\" # Configure the Calico backend to use. calico_backend: \"bird\" # Configure the MTU to use veth_mtu: \"1440\" # The CNI network configuration to install on each node. The special # values in this config will be automatically populated. cni_network_config: | - { \"name\" : \"k8s-pod-network\" , \"cniVersion\" : \"0.3.0\" , \"plugins\" : [ { \"type\" : \"calico\" , \"log_level\" : \"info\" , \"datastore_type\" : \"kubernetes\" , \"nodename\" : \"__KUBERNETES_NODE_NAME__\" , \"mtu\" : __CNI_MTU__, \"ipam\" : { \"type\" : \"host-local\" , \"subnet\" : \"usePodCidr\" } , \"policy\" : { \"type\" : \"k8s\" } , \"kubernetes\" : { \"kubeconfig\" : \"__KUBECONFIG_FILEPATH__\" } } , { \"type\" : \"portmap\" , \"snat\" : true, \"capabilities\" : { \"portMappings\" : true } } ] } --- # This manifest creates a Service, which will be backed by Calico's Typha daemon. # Typha sits in between Felix and the API server, reducing Calico's load on the API server. apiVersion: v1 kind: Service metadata: name: calico-typha namespace: kube-system labels: k8s-app: calico-typha spec: ports: - port: 5473 protocol: TCP targetPort: calico-typha name: calico-typha selector: k8s-app: calico-typha --- # This manifest creates a Deployment of Typha to back the above service. apiVersion: apps/v1beta1 kind: Deployment metadata: name: calico-typha namespace: kube-system labels: k8s-app: calico-typha spec: # Number of Typha replicas. To enable Typha, set this to a non-zero value *and* set the # typha_service_name variable in the calico-config ConfigMap above. # # We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is essential # (when using the Kubernetes datastore). Use one replica for every 100-200 nodes. In # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade. replicas: 0 revisionHistoryLimit: 2 template: metadata: labels: k8s-app: calico-typha annotations: # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical # add-on, ensuring it gets priority scheduling and that its resources are reserved # if it ever gets evicted. scheduler.alpha.kubernetes.io/critical-pod: '' spec: nodeSelector: beta.kubernetes.io/os: linux hostNetwork: true tolerations: # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists # Since Calico can't network a pod until Typha is up, we need to run Typha itself # as a host-networked pod. serviceAccountName: calico-node containers: - image: quay.io/calico/typha:v3.2.4 name: calico-typha ports: - containerPort: 5473 name: calico-typha protocol: TCP env: # Enable \"info\" logging by default. Can be set to \"debug\" to increase verbosity. - name: TYPHA_LOGSEVERITYSCREEN value: \"info\" # Disable logging to file and syslog since those don't make sense in Kubernetes. - name: TYPHA_LOGFILEPATH value: \"none\" - name: TYPHA_LOGSEVERITYSYS value: \"none\" # Monitor the Kubernetes API to find the number of running instances and rebalance # connections. - name: TYPHA_CONNECTIONREBALANCINGMODE value: \"kubernetes\" - name: TYPHA_DATASTORETYPE value: \"kubernetes\" - name: TYPHA_HEALTHENABLED value: \"true\" # Uncomment these lines to enable prometheus metrics. Since Typha is host-networked, # this opens a port on the host, which may need to be secured. #- name: TYPHA_PROMETHEUSMETRICSENABLED # value: \"true\" #- name: TYPHA_PROMETHEUSMETRICSPORT # value: \"9093\" livenessProbe: exec: command: - calico-typha - check - liveness periodSeconds: 30 initialDelaySeconds: 30 readinessProbe: exec: command: - calico-typha - check - readiness periodSeconds: 10 --- # This manifest installs the calico/node container, as well # as the Calico CNI plugins and network config on # each master and worker node in a Kubernetes cluster. kind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. scheduler.alpha.kubernetes.io/critical-pod: '' spec: nodeSelector: beta.kubernetes.io/os: linux hostNetwork: true tolerations: # Make sure calico-node gets scheduled on all nodes. - effect: NoSchedule operator: Exists # Mark the pod as a critical add-on for rescheduling. - key: CriticalAddonsOnly operator: Exists - effect: NoExecute operator: Exists serviceAccountName: calico-node # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods. terminationGracePeriodSeconds: 0 containers: # Runs calico/node container on each Kubernetes node. This # container programs network policy and routes on each # host. - name: calico-node image: quay.io/calico/node:v3.2.4 env: # Use Kubernetes API as the backing datastore. - name: DATASTORE_TYPE value: \"kubernetes\" # Typha support: controlled by the ConfigMap. - name: FELIX_TYPHAK8SSERVICENAME valueFrom: configMapKeyRef: name: calico-config key: typha_service_name # Wait for the datastore. - name: WAIT_FOR_DATASTORE value: \"true\" # Set based on the k8s node name. - name: NODENAME valueFrom: fieldRef: fieldPath: spec.nodeName # Choose the backend to use. - name: CALICO_NETWORKING_BACKEND valueFrom: configMapKeyRef: name: calico-config key: calico_backend # Cluster type to identify the deployment type - name: CLUSTER_TYPE value: \"k8s,bgp\" # Auto-detect the BGP IP address. - name: IP value: \"autodetect\" # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Always\" # Enable IP-in-IP within Felix. - name: FELIX_IPINIPENABLED value: \"true\" # Set MTU for tunnel device used if ipip is enabled - name: FELIX_IPINIPMTU valueFrom: configMapKeyRef: name: calico-config key: veth_mtu # The default IPv4 pool to create on startup if none exists. Pod IPs will be # chosen from this range. Changing this value after installation will have # no effect. This should fall within `--cluster-cidr`. - name: CALICO_IPV4POOL_CIDR value: \"172.20.0.0/16\" # Disable file logging so `kubectl logs` works. - name: CALICO_DISABLE_FILE_LOGGING value: \"true\" # Set Felix endpoint to host default action to ACCEPT. - name: FELIX_DEFAULTENDPOINTTOHOSTACTION value: \"ACCEPT\" # Disable IPv6 on Kubernetes. - name: FELIX_IPV6SUPPORT value: \"false\" # Set Felix logging to \"info\" - name: FELIX_LOGSEVERITYSCREEN value: \"info\" - name: FELIX_HEALTHENABLED value: \"true\" securityContext: privileged: true resources: requests: cpu: 250m livenessProbe: httpGet: path: /liveness port: 9099 host: localhost periodSeconds: 10 initialDelaySeconds: 10 failureThreshold: 6 readinessProbe: exec: command: - /bin/calico-node - -bird-ready - -felix-ready periodSeconds: 10 volumeMounts: - mountPath: /lib/modules name: lib-modules readOnly: true - mountPath: /var/run/calico name: var-run-calico readOnly: false - mountPath: /var/lib/calico name: var-lib-calico readOnly: false # This container installs the Calico CNI binaries # and CNI network config file on each node. - name: install-cni image: quay.io/calico/cni:v3.2.4 command: [ \"/install-cni.sh\" ] env: # Name of the CNI config file to create. - name: CNI_CONF_NAME value: \"10-calico.conflist\" # Set the hostname based on the k8s node name. - name: KUBERNETES_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName # The CNI network config to install on each node. - name: CNI_NETWORK_CONFIG valueFrom: configMapKeyRef: name: calico-config key: cni_network_config # CNI MTU Config variable - name: CNI_MTU valueFrom: configMapKeyRef: name: calico-config key: veth_mtu volumeMounts: - mountPath: /host/opt/cni/bin name: cni-bin-dir - mountPath: /host/etc/cni/net.d name: cni-net-dir volumes: # Used by calico/node. - name: lib-modules hostPath: path: /lib/modules - name: var-run-calico hostPath: path: /var/run/calico - name: var-lib-calico hostPath: path: /var/lib/calico # Used to install CNI. - name: cni-bin-dir hostPath: path: /opt/cni/bin - name: cni-net-dir hostPath: path: /etc/cni/net.d --- apiVersion: v1 kind: ServiceAccount metadata: name: calico-node namespace: kube-system --- # Create all the CustomResourceDefinitions needed for # Calico policy and networking mode. apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: felixconfigurations.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: FelixConfiguration plural: felixconfigurations singular: felixconfiguration --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: bgppeers.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: BGPPeer plural: bgppeers singular: bgppeer --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: bgpconfigurations.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: BGPConfiguration plural: bgpconfigurations singular: bgpconfiguration --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ippools.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: IPPool plural: ippools singular: ippool --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: hostendpoints.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: HostEndpoint plural: hostendpoints singular: hostendpoint --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: clusterinformations.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: ClusterInformation plural: clusterinformations singular: clusterinformation --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: globalnetworkpolicies.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: GlobalNetworkPolicy plural: globalnetworkpolicies singular: globalnetworkpolicy --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: globalnetworksets.crd.projectcalico.org spec: scope: Cluster group: crd.projectcalico.org version: v1 names: kind: GlobalNetworkSet plural: globalnetworksets singular: globalnetworkset --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networkpolicies.crd.projectcalico.org spec: scope: Namespaced group: crd.projectcalico.org version: v1 names: kind: NetworkPolicy plural: networkpolicies singular: networkpolicy rbac.yaml # Calico Version v3.2.4 # https://docs.projectcalico.org/v3.2/releases#v3.2.4 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: calico-node rules: - apiGroups: [ \"\" ] resources: - namespaces - serviceaccounts verbs: - get - list - watch - apiGroups: [ \"\" ] resources: - pods/status verbs: - update - apiGroups: [ \"\" ] resources: - pods verbs: - get - list - watch - patch - apiGroups: [ \"\" ] resources: - services verbs: - get - apiGroups: [ \"\" ] resources: - endpoints verbs: - get - apiGroups: [ \"\" ] resources: - nodes verbs: - get - list - update - watch - apiGroups: [ \"extensions\" ] resources: - networkpolicies verbs: - get - list - watch - apiGroups: [ \"networking.k8s.io\" ] resources: - networkpolicies verbs: - watch - list - apiGroups: [ \"crd.projectcalico.org\" ] resources: - globalfelixconfigs - felixconfigurations - bgppeers - globalbgpconfigs - bgpconfigurations - ippools - globalnetworkpolicies - globalnetworksets - networkpolicies - clusterinformations - hostendpoints verbs: - create - get - list - update - watch --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: calico-node roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: calico-node subjects: - kind: ServiceAccount name: calico-node namespace: kube-system","title":"15.1 \u7f16\u5199Calico Yaml"},{"location":"kubernetes/Installing/#152","text":"\u81ea\u52a8\u914d\u7f6e\u811a\u672c\u8981,\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6,Calico\u653e\u5728\u540c\u7ea7\u76ee\u5f55 setup.sh #/bin/bash function check_parm () { if [ \" ${ 2 } \" == \"\" ] ; then echo -n \" ${ 1 } \" return 1 else return 0 fi } if [ -f ./master-info ] ; then source ./CLUSTER_INFO fi check_parm \"Enter the IP address of master-01: \" ${ CP0_IP } if [ $? -eq 1 ] ; then read CP0_IP fi check_parm \"Enter the Hostname of master-01: \" ${ CP0_HOSTNAME } if [ $? -eq 1 ] ; then read CP0_HOSTNAME fi check_parm \"Enter the IP address of master-02: \" ${ CP1_IP } if [ $? -eq 1 ] ; then read CP1_IP fi check_parm \"Enter the Hostname of master-02: \" ${ CP1_HOSTNAME } if [ $? -eq 1 ] ; then read CP1_HOSTNAME fi check_parm \"Enter the IP address of master-03: \" ${ CP2_IP } if [ $? -eq 1 ] ; then read CP2_IP fi check_parm \"Enter the Hostname of master-03: \" ${ CP2_HOSTNAME } if [ $? -eq 1 ] ; then read CP2_HOSTNAME fi check_parm \"Enter the VIP: \" ${ VIP } if [ $? -eq 1 ] ; then read VIP fi check_parm \"Enter the Net Interface: \" ${ NET_IF } if [ $? -eq 1 ] ; then read NET_IF fi check_parm \"Enter the cluster CIDR: \" ${ CIDR } if [ $? -eq 1 ] ; then read CIDR fi echo \"\"\" cluster-info: master-01: ${ CP0_IP } ${ CP0_HOSTNAME } master-02: ${ CP1_IP } ${ CP1_HOSTNAME } master-03: ${ CP2_IP } ${ CP2_HOSTNAME } VIP: ${ VIP } Net Interface: ${ NET_IF } CIDR: ${ CIDR } \"\"\" echo -n 'Please print \"yes\" to continue or \"no\" to cancle: ' read AGREE while [ \" ${ AGREE } \" ! = \"yes\" ] ; do if [ \" ${ AGREE } \" == \"no\" ] ; then exit 0 ; else echo -n 'Please print \"yes\" to continue or \"no\" to cancle: ' read AGREE fi done mkdir -p ~/ikube/tls HOSTS =( ${ CP0_HOSTNAME } ${ CP1_HOSTNAME } ${ CP2_HOSTNAME } ) IPS =( ${ CP0_IP } ${ CP1_IP } ${ CP2_IP } ) PRIORITY =( 100 50 30 ) STATE =( \"MASTER\" \"BACKUP\" \"BACKUP\" ) HEALTH_CHECK = \"\" for index in 0 1 2 ; do HEALTH_CHECK = ${ HEALTH_CHECK } \"\"\" real_server ${ IPS [ $index ] } 6443 { weight 1 SSL_GET { url { path /healthz status_code 200 } connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } \"\"\" done for index in 0 1 2 ; do host = ${ HOSTS [ ${ index } ] } ip = ${ IPS [ ${ index } ] } echo \"\"\" global_defs { router_id LVS_DEVEL } vrrp_instance VI_1 { state ${ STATE [ ${ index } ] } interface ${ NET_IF } virtual_router_id 80 priority ${ PRIORITY [ ${ index } ] } advert_int 1 authentication { auth_type PASS auth_pass just0kk } virtual_ipaddress { ${ VIP } } } virtual_server ${ VIP } 6443 { delay_loop 6 lb_algo rr lb_kind NAT persistence_timeout 50 protocol TCP ${ HEALTH_CHECK } } \"\"\" > ~/ikube/keepalived- ${ index } .conf scp ~/ikube/keepalived- ${ index } .conf ${ host } :/etc/keepalived/keepalived.conf ssh ${ host } \" systemctl restart keepalived kubeadm reset -f rm -rf /etc/kubernetes/pki/\" if [ ${ index } -ne 0 ] ; then ETCD_MEMBER = \" ${ ETCD_MEMBER } ,\" ETCD_STATUS = \"existing\" else ETCD_MEMBER = \"\" ETCD_STATUS = \"new\" fi ETCD_MEMBER = \" ${ ETCD_MEMBER }${ host } =https:// ${ ip } :2380\" echo \"\"\" apiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.12.2 apiServerCertSANs: - ${ CP0_IP } - ${ CP1_IP } - ${ CP2_IP } - ${ CP0_HOSTNAME } - ${ CP1_HOSTNAME } - ${ CP2_HOSTNAME } - ${ VIP } kubeProxy: config: mode: ipvs etcd: local: extraArgs: listen-client-urls: https://127.0.0.1:2379,https:// ${ ip } :2379 advertise-client-urls: https:// ${ ip } :2379 listen-peer-urls: https:// ${ ip } :2380 initial-advertise-peer-urls: https:// ${ ip } :2380 initial-cluster: ${ ETCD_MEMBER } initial-cluster-state: ${ ETCD_STATUS } serverCertSANs: - ${ host } - ${ ip } peerCertSANs: - ${ host } - ${ ip } networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: ${ CIDR } serviceSubnet: ${ SVCRANGE } \"\"\" > ~/ikube/kubeadm-config-m ${ index } .yaml scp ~/ikube/kubeadm-config-m ${ index } .yaml ${ host } :/etc/kubernetes/kubeadm-config.yaml done kubeadm init --config /etc/kubernetes/kubeadm-config.yaml mkdir -p $HOME /.kube cp -f /etc/kubernetes/admin.conf ${ HOME } /.kube/config ETCD = ` kubectl get pods -n kube-system 2 > & 1 | grep etcd | awk '{print $3}' ` echo \"Waiting for etcd bootup...\" while [ \" ${ ETCD } \" ! = \"Running\" ] ; do sleep 1 ETCD = ` kubectl get pods -n kube-system 2 > & 1 | grep etcd | awk '{print $3}' ` done for index in 1 2 ; do host = ${ HOSTS [ ${ index } ] } ip = ${ IPS [ ${ index } ] } ssh $host \"mkdir -p /etc/kubernetes/pki/etcd\" ssh $host \"mkdir -p $HOME /.kube\" scp /etc/kubernetes/pki/ca.crt $host :/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $host :/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $host :/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $host :/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $host :/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $host :/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $host :/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $host :/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $host :/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $host :~/.kube/config kubectl exec \\ -n kube-system etcd- ${ CP0_HOSTNAME } -- etcdctl \\ --ca-file /etc/kubernetes/pki/etcd/ca.crt \\ --cert-file /etc/kubernetes/pki/etcd/peer.crt \\ --key-file /etc/kubernetes/pki/etcd/peer.key \\ --endpoints = https:// ${ CP0_IP } :2379 \\ member add ${ host } https:// ${ ip } :2380 ssh ${ host } \" kubeadm alpha phase certs all --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig controller-manager --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig scheduler --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubelet config write-to-disk --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubelet write-env-file --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig kubelet --config /etc/kubernetes/kubeadm-config.yaml systemctl restart kubelet kubeadm alpha phase etcd local --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase kubeconfig all --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase controlplane all --config /etc/kubernetes/kubeadm-config.yaml kubeadm alpha phase mark-master --config /etc/kubernetes/kubeadm-config.yaml\" done kubectl apply -f rbac.yaml kubectl apply -f calico.yaml echo \"Cluster create finished.\" echo \"\"\" [req] distinguished_name = req_distinguished_name prompt = yes [ req_distinguished_name ] countryName = Country Name (2 letter code) countryName_value = CN stateOrProvinceName = State or Province Name (full name) stateOrProvinceName_value = Shanghai localityName = Locality Name (eg, city) localityName_value = Shanghai organizationName = Organization Name (eg, company) organizationName_value = Weeknd organizationalUnitName = Organizational Unit Name (eg, section) organizationalUnitName_value = R & D Department commonName = Common Name (eg, your name or your server\\'s hostname) commonName_value = *.multi.io emailAddress = Email Address emailAddress_value = weeknd.su@hotmail.com \"\"\" > ~/ikube/tls/openssl.cnf openssl req -newkey rsa:4096 -nodes -config ~/ikube/tls/openssl.cnf -days 3650 -x509 -out ~/ikube/tls/tls.crt -keyout ~/ikube/tls/tls.key kubectl create -n kube-system secret tls ssl --cert ~/ikube/tls/tls.crt --key ~/ikube/tls/tls.key for index in 0 1 2 ; do host = ${ HOSTS [ ${ index } ] } ssh ${ host } \"sed -i 's/etcd-servers=https:\\/\\/127.0.0.1:2379/etcd-servers=https:\\/\\/ ${ CP0_IP } :2379,https:\\/\\/ ${ CP1_IP } :2379,https:\\/\\/ ${ CP2_IP } :2379/g' /etc/kubernetes/manifests/kube-apiserver.yaml\" ssh ${ host } \"sed -i 's/ ${ CP0_IP } / ${ VIP } /g' ~/.kube/config\" done echo \"Plugin install finished.\" echo \"Waiting for all pods into 'Running' statu. You can press 'Ctrl + c' to terminate this waiting any time you like.\" POD_UNREADY = ` kubectl get pods -n kube-system 2 > & 1 | awk '{print $3}' | grep -vE 'Running|STATUS' ` NODE_UNREADY = ` kubectl get nodes 2 > & 1 | awk '{print $2}' | grep 'NotReady' ` while [ \" ${ POD_UNREADY } \" ! = \"\" -o \" ${ NODE_UNREADY } \" ! = \"\" ] ; do sleep 1 POD_UNREADY = ` kubectl get pods -n kube-system 2 > & 1 | awk '{print $3}' | grep -vE 'Running|STATUS' ` NODE_UNREADY = ` kubectl get nodes 2 > & 1 | awk '{print $2}' | grep 'NotReady' ` done","title":"15.2 \u7f16\u5199\u81ea\u52a8\u914d\u7f6e\u811a\u672c"},{"location":"kubernetes/Installing/#153","text":"Bash sh setup.sh","title":"15.3 \u8fd0\u884c\u811a\u672c"},{"location":"kubernetes/Installing/#_1","text":"","title":"\u914d\u7f6e\u7ed3\u675f"},{"location":"syntax/headline/","text":"","title":"Headline"},{"location":"syntax/main/","text":"","title":"Main"},{"location":"syntax/paragraph/","text":"","title":"3. network"}]}